[
{
	"uri": "http://localhost:1313/awsDeployment/2-prerequiste/2.1-create-iam-role/",
	"title": "Create IAM Role for AWS Lambda",
	"tags": [],
	"description": "",
	"content": "This section provides step-by-step instructions to create an IAM Role for the architecture described in the diagram.\nThis IAM Role will allow the Lambda function to read from DynamoDB Streams and write logs to CloudWatch.\nOnce you complete this step, the architecture will look as follows:\nCreate IAM Role Go to Create IAM Role Click IAM. Click Create Role. Select AWS Service, then choose Lambda as the trusted entity. Go to Add Permissions Select AWSLambdaBasicExecutionRole. Select AmazonDynamoDBFullAccess. Click Next.\nGo to Name, Review, and Create\nEnter the name data-streaming-system-role. Click Create Role. Contents Create DynamoDB "
},
{
	"uri": "http://localhost:1313/awsDeployment/4-error-handling/4.1-exception-handling-in-lambda/",
	"title": "Exception Handling in Lambda",
	"tags": [],
	"description": "",
	"content": "In Lambda functions that process data from DynamoDB Streams, errors are inevitable‚Äîespecially when the data is malformed, missing required fields, or encounters issues connecting to other services. Without proper exception handling, the entire process may stop or repeat unnecessarily.\nSome principles for handling exceptions in Lambda:\nWrap your logic with try...catch (Node.js) or try...except (Python) to capture runtime errors.\nLog detailed error information to assist debugging.\nAvoid completely swallowing errors ‚Äî return information or rethrow them so the system can retry if necessary.\nClassify errors: temporary errors can be retried; permanent errors (data-related) should be sent to a Dead Letter Queue (DLQ) for manual processing.\nExample in Node.js:\nexport const handler = async (event) =\u0026gt; { for (const record of event.Records) { try { console.log(\u0026#39;Event Name:\u0026#39;, record.eventName); const newImage = record.dynamodb?.NewImage; if (!newImage) { throw new Error(\u0026#34;Missing NewImage in record\u0026#34;); } const orderId = newImage.OrderID?.S; const product = newImage.Product?.S; const quantity = parseInt(newImage.Quantity?.N); if (!orderId || !product || isNaN(quantity)) { throw new Error(`Invalid data: ${JSON.stringify(newImage)}`); } const transformed = { orderId, product, totalCost: quantity * 10, }; console.log(\u0026#39;Transformed Order:\u0026#39;, transformed); // Write data to another table or send it to another system... } catch (err) { console.error(\u0026#34;Error processing record:\u0026#34;, err); // You may send the failed data to a DLQ or another service for later processing } } return { statusCode: 200, body: \u0026#39;Processing completed\u0026#39; }; }; Notes:\nconsole.error() will log the message as an error in CloudWatch, making it easier to filter and analyze.\nIf you want Lambda to automatically retry, you can rethrow the error instead of swallowing it inside catch.\nFor permanent data errors, configure a DLQ to store and process them manually.\n"
},
{
	"uri": "http://localhost:1313/awsDeployment/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Real-Time Data Processing with DynamoDB Streams and AWS Lambda is a modern serverless architecture that enables you to build systems that respond instantly to changes in your database without managing any servers. This model fully leverages the scalability, high availability, and cost-efficiency of AWS services such as DynamoDB, Lambda, and CloudWatch.\nWhen applying this architecture, you gain several benefits:\nReal-time processing: Any changes (Insert, Update, Delete) in a DynamoDB table can immediately trigger a Lambda function for processing, making it ideal for low-latency applications such as analytics, alerting, and data synchronization.\nServerless \u0026amp; easy deployment: No server management required. Simply configure a trigger between DynamoDB and Lambda to deploy an event processing pipeline.\nFlexible scalability: Lambda automatically scales according to the number of records from the stream. You don‚Äôt need to worry about over-provisioning or under-provisioning as with traditional systems.\nDecoupled processing logic: Tasks such as filtering, transformation, aggregation, sending notifications, or storing additional data in S3/RDS can be handled by individual Lambda functions or a chain of Lambdas combined with EventBridge or Step Functions.\nPerformance optimization: By leveraging batch processing, memory tuning, and concurrent execution, the system can process thousands of events per second while maintaining high performance.\nEasy monitoring: Integrated with CloudWatch to track logs, set up dashboards, configure error alerts, and analyze performance in detail at each stage of processing.\nCost efficiency: Pay only for the number of Lambda executions and DynamoDB storage. Combined with provisioned throughput, you can reduce costs by over 30% compared to traditional solutions.\nEasy testing and operations: You can simulate event flows for testing, write unit tests, and set up robust operational procedures for incident recovery.\nBy combining DynamoDB Streams and AWS Lambda, this architecture delivers a powerful, flexible, and highly scalable solution‚Äîespecially suited for real-time data processing systems such as log analytics platforms, user behavior tracking, instant response systems, and many other modern cloud-based applications on AWS.\n"
},
{
	"uri": "http://localhost:1313/awsDeployment/3-transformation-logic/3.1-create-s3-uploadimage/",
	"title": "Processing Data in Lambda",
	"tags": [],
	"description": "",
	"content": "In a real-time processing system, receiving and transforming data from an event stream is a crucial step to ensure that the data is analyzed and used for the right purposes. AWS DynamoDB Stream allows you to monitor every change in a table, and AWS Lambda acts as the \u0026ldquo;listener\u0026rdquo; that processes the data immediately when a change occurs.\nIn this section, we will implement the transformation logic inside the Lambda function. Data from the DynamoDB Stream will be retrieved, decoded, and transformed according to specific business requirements, laying the foundation for subsequent processing steps such as aggregation, storage, analysis, or sending to other systems.\nGo to the Lambda Function you created: Select the function you created, for example: DynamoStreamProcessor Scroll down to Code source Paste the following code: export const handler = async (event) =\u0026gt; { for (const record of event.Records) { console.log(\u0026#39;Event Name:\u0026#39;, record.eventName); const newImage = record.dynamodb?.NewImage; if (newImage) { const orderId = newImage.OrderID?.S; const product = newImage.Product?.S; const quantity = newImage.Quantity?.N; console.log(`New Order Received - ID: ${orderId}, Product: ${product}, Quantity: ${quantity}`); // Example transformation: calculate order value (assuming each item costs 10) const transformed = { orderId, product, totalCost: Number(quantity) * 10, }; console.log(\u0026#39;Transformed Order:\u0026#39;, transformed); } } return { statusCode: 200, body: \u0026#39;Stream processed successfully\u0026#39; }; }; Explanation: event.Records: A list of events from DynamoDB (each change may send multiple records).\nrecord.eventName: Specifies the type of change (INSERT, MODIFY, REMOVE).\nrecord.dynamodb.NewImage: The new data after the change.\ntransformed variable: The place where you implement your data transformation logic.\nTesting the Data To verify that the Lambda function is working according to the logic you wrote, we will create new data in the DynamoDB table and check whether the Lambda function is triggered.\nCreate a new record in the DynamoDB table Go to AWS Console ‚Üí DynamoDB ‚Üí select the table you created (OrdersTable)\nSelect Explore table items\nClick Create item\nChoose JSON view\nAdd a new record\nClick Create item\nCheck the Lambda execution results Go to AWS Console \u0026gt; CloudWatch\nIn the left menu, choose: Logs ‚Üí Log groups\nFind the log group by your Lambda function name: /aws/lambda/DynamoStreamProcessor\nClick on that log group.\nSelect the most recent log stream ‚Äì corresponding to the time you created the item in DynamoDB.\nYou will see multiple log lines similar to this:\nMeaning:\nüì• Event Name: Indicates whether the action was INSERT, MODIFY, or REMOVE.\nüü¢ New Order Received: Data was successfully read from the DynamoDB stream.\nüîÅ Transformed Order: The data was successfully processed according to the logic you wrote in the Lambda function.\nREPORT: Summary of processing time, memory usage, etc.\n"
},
{
	"uri": "http://localhost:1313/awsDeployment/",
	"title": "Session Management",
	"tags": [],
	"description": "",
	"content": "DynamoDB Triggers \u0026amp; Stream Processing with Lambda Overall In this project, you will explore and implement a real-time data processing system using DynamoDB Streams and AWS Lambda. Whenever there is a change in the DynamoDB table (insert, update, delete), the event will be recorded in the stream and immediately trigger a Lambda function for processing. The system will perform data transformation, aggregation, error handling, and log the results to services such as S3, RDS, or send them to monitoring systems.\nThe main goal is to build a fully serverless architecture that ensures automatic scalability, high performance, cost optimization, and easy maintenance. You will also implement techniques such as performance monitoring, test framework design, and error operation procedures to complete a modern and practical stream data processing system.\nContent Introduction Preparation Data Transformation Logic Error Handling Clean Up Resources "
},
{
	"uri": "http://localhost:1313/awsDeployment/2-prerequiste/2.2-create-dynamo/",
	"title": "Create DynamoDB Table and Enable DynamoDB Stream",
	"tags": [],
	"description": "",
	"content": "Create DynamoDB To process real-time data in a Serverless system, we need a mechanism to listen for changes happening in the database.\nDynamoDB Streams is the tool that supports this goal.\nIn this step, we will create a DynamoDB table to store data and enable the Stream feature, allowing services such as AWS Lambda to be automatically triggered whenever there are changes (insert, update, delete) in the table.\nEnabling DynamoDB Streams helps you build a powerful real-time data processing pipeline, such as logging, data synchronization, statistical calculations, or transforming data into another system.\nThis is an essential foundation for event-driven architecture.\nGo to DynamoDB\nClick DynamoDB. Click Tables, then click Create table. In the table creation form: Set the table name to OrdersTable Set the partition key to OrderID Select Customize settings Then click Create table. Enable DynamoDB Stream Click on the table you just created. Select Exports and Streams. In the DynamoDB stream details section, click Turn on. Choose New and old images. Click Turn on stream. "
},
{
	"uri": "http://localhost:1313/awsDeployment/3-transformation-logic/3.2-create-rds/",
	"title": "Data Aggregation",
	"tags": [],
	"description": "",
	"content": "In real-time data processing systems, merely recording individual events is not enough.\nA common real-world requirement is to aggregate data to provide generalized insights, enabling faster analysis and decision-making.\nIn this project, whenever a new order is recorded in the OrdersTable, the system not only processes that event but also updates the total quantity of each product ordered. This offers a more intuitive view of consumer trends in real time without having to run batch jobs at the end of the day like traditional systems.\nTo achieve this, we implement the aggregation logic directly inside the Lambda function using DynamoDB UpdateItem with the ADD expression. This ensures high performance and strong consistency. All changes are recorded instantly, matching the characteristics of modern serverless event-driven architectures.\nCreate a new aggregation table Create Table\nGo to DynamoDB \u0026gt; Tables \u0026gt; Create Table Enter the following details:\nTable name: AggregateTable\nPartition key: product (String type)\nClick Create Table\nUpdate Lambda to write data into the table Reopen the Lambda function that processes the stream. Add write permissions to AggregateTable: Go to Configuration \u0026gt; Permissions \u0026gt; Rolename (e.g., LambdaDynamoDBStreamRole) In the IAM Role page, choose Add permissions \u0026gt; Attach policies Attach the AmazonDynamoDBFullAccess policy (or a custom policy allowing only PutItem and UpdateItem on the AggregateTable).\nThen go back to the Lambda function and update the code as follows:\nimport { DynamoDBClient, UpdateItemCommand } from \u0026#34;@aws-sdk/client-dynamodb\u0026#34;; const client = new DynamoDBClient({}); export const handler = async (event) =\u0026gt; { for (const record of event.Records) { console.log(\u0026#34;üì• Event Name:\u0026#34;, record.eventName); const newImage = record.dynamodb?.NewImage; if (!newImage) continue; const orderId = newImage.OrderID?.S || \u0026#34;unknown\u0026#34;; const product = newImage.Product?.S || \u0026#34;unknown\u0026#34;; const quantity = parseInt(newImage.Quantity?.N || \u0026#34;0\u0026#34;); console.log(`üõí New Order Received - ID: ${orderId}, Product: ${product}, Quantity: ${quantity}`); const transformedOrder = { orderId, product, quantity }; console.log(\u0026#34;üîÅ Transformed Order:\u0026#34;, transformedOrder); const params = { TableName: \u0026#34;AggregateTable\u0026#34;, Key: { Product: { S: product } }, UpdateExpression: \u0026#34;SET totalQuantity = if_not_exists(totalQuantity, :zero) + :qty\u0026#34;, ExpressionAttributeValues: { \u0026#34;:qty\u0026#34;: { N: quantity.toString() }, \u0026#34;:zero\u0026#34;: { N: \u0026#34;0\u0026#34; } }, ReturnValues: \u0026#34;UPDATED_NEW\u0026#34; }; try { const result = await client.send(new UpdateItemCommand(params)); const newTotal = result.Attributes?.totalQuantity?.N; console.log(`üìä Total quantity after aggregation: ${newTotal}`); console.log(`‚úÖ Successfully aggregated into AggregateTable for product ${product}`); } catch (err) { console.error(\u0026#34;‚ùå Error updating AggregateTable:\u0026#34;, err); } } return { statusCode: 200, body: \u0026#34;OK\u0026#34; }; }; Test the functionality:\nGo to OrdersTable \u0026gt; Items \u0026gt; Create Item Create a new item with OrderID, Product, Quantity Select JSON view { \u0026#34;OrderID\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;order-1\u0026#34; }, \u0026#34;Product\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Notebook\u0026#34; }, \u0026#34;Quantity\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;5\u0026#34; } } Wait 1‚Äì2 seconds ‚Üí Open AggregateTable, choose Scan \u0026gt; Run Verify that the totalQuantity column has been correctly updated for the corresponding Product. "
},
{
	"uri": "http://localhost:1313/awsDeployment/4-error-handling/4.2-log-errors-with-cloudwatch/",
	"title": "Logging Errors with CloudWatch",
	"tags": [],
	"description": "",
	"content": "When the system encounters an error, recording detailed information about it is a crucial step for root cause analysis and troubleshooting.\nAWS CloudWatch is the default logging and monitoring service for Lambda, allowing you to easily track the entire processing flow, including when errors occur.\nHow to Log Errors In your Lambda source code, you can use:\nconsole.error() to log errors. console.warn() to log warnings. console.log() for general information. Example:\ntry { // Data processing logic } catch (error) { console.error(\u0026#34;‚ùå Error processing record:\u0026#34;, error); console.error(\u0026#34;üìÑ Stack trace:\u0026#34;, error.stack); } Viewing logs in CloudWatch\nOpen AWS Management Console.\nGo to CloudWatch ‚Üí Log groups.\nFind the log group by the Lambda function name, e.g. /aws/lambda/DynamoStreamProcessor.\nSelect the latest log stream to see detailed error information.\nImportance of logging errors\nQuickly identify root causes: Determine if the error is caused by input data, service connection issues, or processing logic.\nTrack error frequency: See whether the error happens sporadically or repeatedly.\nEnable automatic alerts: You can integrate with CloudWatch Alarms to send email/SMS notifications when the number of errors exceeds a threshold.\n"
},
{
	"uri": "http://localhost:1313/awsDeployment/2-prerequiste/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": "To deploy a real-time data processing system using DynamoDB Triggers and AWS Lambda, you need to perform several essential preparation steps to ensure the proper permissions and environment setup on AWS.\nIn this preparation phase, we will create an IAM Role to grant AWS Lambda the necessary permissions to access services such as DynamoDB, CloudWatch Logs, and other services that support data processing, storage, and monitoring.\nAfter configuring the IAM Role, we will proceed to create a DynamoDB table and set up a streaming trigger with AWS Lambda.\nContents Create IAM Role Create DynamoDB Table and Enable DynamoDB Stream Create Lambda and Attach to DynamoDB Stream "
},
{
	"uri": "http://localhost:1313/awsDeployment/2-prerequiste/2.3-create-lambda/",
	"title": "Create Lambda and Attach to DynamoDB Stream",
	"tags": [],
	"description": "",
	"content": "After creating the DynamoDB table and enabling the Stream feature, the next step is to build a Lambda function to process the events recorded from that Stream.\nAWS Lambda allows you to write code that executes in response to data changes in DynamoDB ‚Äî such as when a new record is added, updated, or deleted.\nIn this step, we will create a Lambda function using the appropriate programming language (e.g., Python or Node.js), then configure it to be automatically triggered by events from the DynamoDB Stream.\nThis enables the system to process real-time data without managing servers and scale easily when traffic increases.\nBenefits:\nReal-time processing: Data inserted or updated in DynamoDB is processed immediately via Lambda. No infrastructure management: Lambda is a serverless service, so you don‚Äôt need to operate any servers. Seamless integration: Connecting DynamoDB and Lambda via Streams is natively supported by AWS with a simple setup. Once created, you can implement custom logic such as sending notifications, logging, synchronizing with other services, or executing domain-specific business operations.\nCreate Lambda Function Go to Create Lambda Function Click Lambda. Click Create a function. Select: Author from scratch Function name: DynamoStreamProcessor Runtime: Node.js 22.x Architecture: x86_64 Use an existing role ‚Üí Select the previously created LambdaDynamoDBStreamRole Attach DynamoDB Stream Trigger to the Lambda Function After creating the Lambda function and enabling DynamoDB Streams for the table, the next step is to configure Lambda so that it is automatically triggered whenever the DynamoDB table has data changes (insert, update, delete).\nThis connects the DynamoDB table (with Streams enabled) to Lambda for processing real-time data whenever there are changes.\nOpen the Lambda function you just created: Go to Lambda. Select the newly created function: DynamoStreamProcessor. Click Add trigger. Choose the service: DynamoDB. Configure the trigger:\nTable: OrdersTable (or the name of your created table) Batch size: Keep the default value 100 Starting position: Select LATEST (only receive new data) Then click Add.\nAfter completion: "
},
{
	"uri": "http://localhost:1313/awsDeployment/3-transformation-logic/",
	"title": "Data Transformation Logic",
	"tags": [],
	"description": "",
	"content": "In a real-time data processing system, receiving data from a source (such as DynamoDB Streams) is only the first step. To create value from this data stream, we need to perform a transformation process‚Äîconverting raw data into a format suitable for processing, storage, or display.\nThe Lambda function plays the central role in this process. Whenever a change occurs in the DynamoDB table, an event record will be sent to the Lambda function. Here, the data is extracted, analyzed, and processed according to specific business logic. This is a crucial step that makes the system intelligent, flexible, and scalable in real-world scenarios.\nContents Processing Data in Lambda Data Aggregation Logging and Monitoring with CloudWatch "
},
{
	"uri": "http://localhost:1313/awsDeployment/3-transformation-logic/3.3-log-and-monitor-with-cloudwatch/",
	"title": "Logging and Monitoring Processing with CloudWatch",
	"tags": [],
	"description": "",
	"content": "During system operation, logging plays a crucial role in tracking, analyzing, and troubleshooting issues. AWS provides the Amazon CloudWatch service, which allows collecting, storing, and monitoring logs from other services such as AWS Lambda, DynamoDB, and more.\nWhen Lambda processes data from a DynamoDB Stream, all relevant information is programmed to be logged via console.log() statements (or equivalent) in the source code. These logs include:\nInformation about the received event (Event Name, Order ID, Product, Quantity, etc.)\nData after transformation (Transformed Order).\nProcessing start and end times, along with performance metrics (execution time, memory usage).\nExample log output displayed in CloudWatch as shown in the figure below:\nLogging helps to:\nTrack Lambda\u0026rsquo;s processing in real-time.\nQuickly analyze errors when issues occur.\nEvaluate the performance of the Lambda function and optimize operational costs.\nThanks to CloudWatch, the team can ensure that the data processing system operates stably, transparently, and is easy to maintain.\n"
},
{
	"uri": "http://localhost:1313/awsDeployment/4-error-handling/4.3-dead-letter-queue/",
	"title": "Using Dead Letter Queue (DLQ) to Store Failed Events",
	"tags": [],
	"description": "",
	"content": "In real-time data processing systems, when a Lambda function repeatedly encounters errors (e.g., invalid data or an unavailable destination service), improper handling could lead to lost data or a bottleneck in the entire processing flow.\nAWS provides Dead Letter Queue (DLQ) to store events that fail to be processed successfully, allowing you to analyze and reprocess them later.\nHow it works When Lambda fails to process an event after the maximum retry attempts, the event is sent to a DLQ (usually an SQS queue or an SNS topic).\nDLQ separates failed events from the main processing stream, preventing them from affecting valid events.\nHow to configure DLQ for Lambda Open AWS Lambda Console ‚Üí select the function you want to configure.\nGo to the Configuration tab ‚Üí Asynchronous invocation.\nIn the Dead-letter queue section, select:\nA pre-created SQS queue or SNS topic. Save the configuration.\nExample scenario Suppose Lambda reads data from a DynamoDB Stream but encounters a record missing a required field:\nLambda retries according to the configuration. If it still fails, that record is sent to an SQS DLQ. You can create another Lambda to read from the DLQ to manually process the record or send alerts. Benefits of DLQ:\nNo data loss: Every failed record is retained. Easy reprocessing: You can rerun the logic or fix the data. Improved stability: Prevents issues from spreading to other valid events. "
},
{
	"uri": "http://localhost:1313/awsDeployment/4-error-handling/4.4-error-using-cloudwatch-alarm/",
	"title": "Automatic Error Alerts with CloudWatch Alarm",
	"tags": [],
	"description": "",
	"content": "Logging errors or storing them in a DLQ is not enough if the operations team is not notified in time.\nTo proactively detect incidents, AWS provides CloudWatch Alarm combined with SNS to send alerts via email, SMS, or chat systems like Slack.\nHow it works CloudWatch Metrics records the number of errors or failed Lambda executions. CloudWatch Alarm monitors these metrics. When a predefined threshold is exceeded, the Alarm triggers an action to send a notification via SNS. Steps to set up alerts Create an SNS Topic for notifications\nOpen the SNS Console ‚Üí Create topic ‚Üí choose Standard. Name the topic, create it, then Create subscription to register an email or SMS endpoint. Confirm the subscription via the email received. Create a CloudWatch Alarm\nOpen CloudWatch Console ‚Üí Alarms ‚Üí Create alarm. Select Lambda metrics ‚Üí Errors or DeadLetterErrors. Set the threshold, e.g., more than 3 errors in 5 minutes. In Actions, select the SNS Topic created earlier. Save and activate the alarm\nExample scenario If Lambda fails 5 consecutive times within 10 minutes, CloudWatch Alarm will immediately send an email to the operations team. The team can then check logs or the DLQ to find and resolve the issue. Benefits:\nQuick response: Reduce the time to detect incidents. Lower risk of data loss: Take early action when errors occur in bulk. Automated monitoring: No need to manually check logs. "
},
{
	"uri": "http://localhost:1313/awsDeployment/4-error-handling/",
	"title": "Error Handling",
	"tags": [],
	"description": "",
	"content": "In real-time data processing systems, errors are inevitable. They can arise from invalid input data, network issues, insufficient permissions, or unexpected conditions in processing logic. If not handled properly, these errors can disrupt workflows, cause data loss, or produce inconsistent results.\nIn this context, error handling is particularly critical because the Lambda function processes data directly from the DynamoDB Stream. A single unhandled erroneous record can cause the system to retry processing or block subsequent events.\nTo ensure a stable and reliable system, AWS provides several error-handling mechanisms, such as:\nLogging with CloudWatch ‚Äì Store error details for analysis and resolution. Retry and backoff mechanism ‚Äì Automatically retry failed events with increasing delays. Dead Letter Queue (DLQ) ‚Äì Store repeatedly failed events for manual processing later. Custom exception handling ‚Äì Validate, filter, and transform data to avoid unnecessary errors. By implementing these methods, the system can recover from errors, maintain data consistency, and reduce operational costs.\nContents: Exception Handling in Lambda Logging Errors with CloudWatch Using Dead Letter Queue (DLQ) to Store Errors Automatic Error Alerts with CloudWatch "
},
{
	"uri": "http://localhost:1313/awsDeployment/5-cleanup/",
	"title": "Clean Up Resources",
	"tags": [],
	"description": "",
	"content": "After completing testing or deployment, cleaning up unused AWS resources is crucial to:\nAvoid unnecessary costs. Reduce security risks from unused services or permissions. Keep your AWS account organized and easier to manage. Below is a detailed guide to delete each type of resource used in the project.\nDelete Lambda Functions Go to Lambda Select the Functions you want to delete. Choose Actions ‚Üí Delete. Type confirm ‚Üí Select Delete. Delete DynamoDB Tables and Streams Go to DynamoDB Select the tables you created. Choose Delete. Type confirm ‚Üí Delete. üí° Note: If the table has DynamoDB Streams enabled, deleting the table will also delete the associated stream and remove any related Lambda triggers.\nDelete SQS/SNS (DLQ) If your project used a Dead Letter Queue (DLQ):\nOpen SQS (if DLQ is an SQS queue): Select the queue you want to delete. Click Delete and confirm. Open SNS (if DLQ is an SNS topic): Select the topic you want to delete. Click Delete and confirm. Delete CloudWatch Log Groups Go to CloudWatch Choose Log groups. Find the log group matching your Lambda function name, e.g., /aws/lambda/DynamoStreamProcessor. Select the log group ‚Üí Actions ‚Üí Delete log group. Click Delete to confirm. Delete IAM Roles Go to IAM Roles Select the role(s) used by your Lambda or other services in the project. Click Delete. Type delete ‚Üí Click Delete to confirm. "
},
{
	"uri": "http://localhost:1313/awsDeployment/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/awsDeployment/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]